import * as React from 'react';
import { useSettingsContext } from './SettingsContext';
import { useWebSocketBridge, WebSocketMessageType } from './WebSocketContextBridge';
import axios from 'axios';

// Audio context interfaces

// Define interfaces for audio context
export interface AudioDevice {
  id: string;
  name: string;
  isInput: boolean;
  isOutput: boolean;
  isLoopback: boolean;
  isDefault: boolean;
  // Additional properties for Web Audio API
  deviceId?: string;
  groupId?: string;
  kind?: MediaDeviceKind;
  label?: string;
}

export interface RecordingStatus {
  isRecording: boolean;
  sessionId: string | null;
  startTime: string | null;
  duration: number;
  audioLevel: number;
  errorMessage: string | null;
}

interface AudioContextType {
  devices: AudioDevice[];
  selectedDevice: AudioDevice | null;
  isLoading: boolean;
  error: string | null;
  recordingStatus: RecordingStatus | null;
  refreshDevices: () => Promise<void>;
  selectDevice: (device: AudioDevice) => void;
  startRecording: (title: string, description?: string) => Promise<string | null>;
  stopRecording: (sessionId: string) => Promise<boolean>;
  getAudioLevel: () => number;
  uploadAudio: (file: File, title: string, description?: string) => Promise<string | null>;
}

const AudioContext = React.createContext<AudioContextType | undefined>(undefined);

export const useAudio = () => {
  const context = React.useContext(AudioContext);
  if (context === undefined) {
    throw new Error('useAudio must be used within an AudioProvider');
  }
  return context;
};

export const AudioProvider = ({ children }: { children: React.ReactNode }) => {
  const [devices, setDevices] = React.useState<AudioDevice[]>([]);
  const [selectedDevice, setSelectedDevice] = React.useState<AudioDevice | null>(null);
  const [isLoading, setIsLoading] = React.useState<boolean>(false);
  const [error, setError] = React.useState<string | null>(null);
  const [recordingStatus, setRecordingStatus] = React.useState<RecordingStatus | null>(null);
  
  // Keep a reference to the active session ID
  const audioLevelRef = React.useRef<number>(0.05);
  const statusPollingRef = React.useRef<NodeJS.Timeout | null>(null);
  
  // Audio processing references
  const mediaStreamRef = React.useRef<MediaStream | null>(null);
  const audioContextRef = React.useRef<AudioContext | null>(null);
  const analyserRef = React.useRef<AnalyserNode | null>(null);
  const sourceNodeRef = React.useRef<MediaStreamAudioSourceNode | null>(null);
  const processorNodeRef = React.useRef<ScriptProcessorNode | null>(null);
  const audioBufferRef = React.useRef<Float32Array | null>(null);
  const audioChunksRef = React.useRef<Blob[]>([]);
  const webSocketStreamingRef = React.useRef<boolean>(false);
  const lastChunkSentTimeRef = React.useRef<number>(0);

  const { settings } = useSettingsContext();
  // Get the WebSocket context for audio streaming
  const webSocketContext = useWebSocketBridge();

  // Initialize with devices on startup
  React.useEffect(() => {
    refreshDevices();
  }, []);

  // Simulated audio level for recordings
  React.useEffect(() => {
    if (recordingStatus?.isRecording) {
      const simulateAudioLevel = setInterval(() => {
        audioLevelRef.current = 0.05 + Math.random() * 0.75;
      }, 100);
      return () => clearInterval(simulateAudioLevel);
    }
  }, [recordingStatus?.isRecording]);

  const refreshDevices = async () => {
    setIsLoading(true);
    setError(null);

    try {
      // Check if browser supports mediaDevices API
      if (navigator.mediaDevices && navigator.mediaDevices.enumerateDevices) {
        // Request permission to access audio devices
        await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // Get available media devices
        const mediaDevices = await navigator.mediaDevices.enumerateDevices();
        const audioDevices = mediaDevices.filter(device => device.kind === 'audioinput');
        
        // Transform to our AudioDevice interface
        const transformedDevices: AudioDevice[] = audioDevices.map((device, index) => {
          return {
            id: device.deviceId || `device-${index}`,
            name: device.label || `Microphone ${index + 1}`,
            isInput: true,
            isOutput: false,
            isLoopback: device.label.toLowerCase().includes('loopback') || device.label.toLowerCase().includes('system audio'),
            isDefault: device.deviceId === 'default',
            deviceId: device.deviceId,
            groupId: device.groupId,
            kind: device.kind,
            label: device.label
          };
        });
        
        // Add system audio option if on electron
        if (window.electron && window.electron.getSystemAudio) {
          transformedDevices.push({
            id: 'system-audio',
            name: 'System Audio',
            isInput: true,
            isOutput: false,
            isLoopback: true,
            isDefault: false
          });
        }
        
        setDevices(transformedDevices);
        
        // Select default device if none selected
        if (!selectedDevice && transformedDevices.length > 0) {
          // Prefer loopback device for best experience
          const loopbackDevice = transformedDevices.find(d => d.isLoopback);
          const defaultDevice = transformedDevices.find(d => d.isDefault);
          selectDevice(loopbackDevice || defaultDevice || transformedDevices[0]);
        }
      } else {
        console.error('MediaDevices API not supported in this browser');
        setError('Your browser does not support audio recording. Please use a modern browser like Chrome or Firefox.');
      }
    } catch (err) {
      console.error('Error accessing media devices:', err);
      setError('Failed to access your microphone. Please check permissions and try again.');
    } finally {
      setIsLoading(false);
    }
  };

  const selectDevice = (device: AudioDevice) => {
    setSelectedDevice(device);
  };

  // Audio processing callback
  const onaudioprocess = (event: AudioProcessingEvent) => {
    try {
      // Don't process if streaming is disabled
      if (!webSocketStreamingRef.current) {
        console.debug('Audio streaming is disabled, not processing audio');
        return;
      }
      
      // Skip if no session ID is available
      if (!recordingStatus?.sessionId) {
        console.debug('No active recording session ID, not sending audio');
        return;
      }
      
      // Get the raw audio data from the input channel
      const inputBuffer = event.inputBuffer;
      const inputData = inputBuffer.getChannelData(0);
      
      // Calculate RMS for audio level visualization
      let sum = 0;
      for (let i = 0; i < inputData.length; i++) {
        sum += inputData[i] * inputData[i];
      }
      const rms = Math.sqrt(sum / inputData.length);
      audioLevelRef.current = Math.min(1.0, rms * 5); // Scale up for better visualization
      
      // Check if we need to attempt reconnection of WebSocket
      if (webSocketContext.connectionStatus !== 'open') {
        console.log('WebSocket disconnected during recording, attempting to reconnect...');
        webSocketContext.reconnect();
        return; // Skip this frame until reconnected
      }
      
      // Create a copy of the data for processing
      const audioChunk = inputData.slice(0);
      
      // Convert Float32Array to regular array for JSON serialization
      const audioDataArray = Array.from(audioChunk);
      
      // Sample down to reduce data size (keep only every 3rd sample for better performance)
      const sampledData = [];
      const samplingRate = 3; // Take every 3rd sample
      for (let i = 0; i < audioDataArray.length; i += samplingRate) {
        sampledData.push(audioDataArray[i]);
      }
      
      // Throttle sending to avoid overwhelming the WebSocket
      const now = Date.now();
      if (now - lastChunkSentTimeRef.current > 100) { // Send more frequently (every 100ms)
        lastChunkSentTimeRef.current = now;
        
        // Create a more compact representation of the audio data
        // We'll use a simple JSON string format for maximum compatibility
        const audioData = JSON.stringify(sampledData);
        
        // Send the audio chunk message directly
        console.log(`Sending audio chunk for session ${recordingStatus.sessionId}, length: ${sampledData.length}`);
        const success = webSocketContext.sendMessage({
          type: 'audio_chunk' as WebSocketMessageType,
          data: {
            session_id: recordingStatus.sessionId,
            timestamp: now,
            chunk_index: audioChunksRef.current.length,
            sample_rate: audioContextRef.current?.sampleRate || 44100,
            format: 'json',
            channels: 1,
            audio_data: audioData
          }
        });
        
        if (!success) {
          console.warn('Failed to send audio chunk, WebSocket may be disconnected');
        } else {
          // Log occasionally for debugging
          if (audioChunksRef.current.length % 20 === 0) {
            console.log(`Sent ${audioChunksRef.current.length} audio chunks so far`);
          }
          
          // Save the audio chunk locally (for potential local playback or download)
          try {
            const audioBlob = new Blob([audioChunk], { type: 'audio/wav' });
            audioChunksRef.current.push(audioBlob);
          } catch (error) {
            console.error('Error creating audio blob:', error);
          }
        }
      }
    } catch (error) {
      console.error('Error processing audio:', error);
    }
  };

  // Clean up audio processing resources
  const cleanupAudioProcessing = () => {
    console.log('Cleaning up audio processing resources');
    
    // Stop all tracks in the media stream
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => {
        track.stop();
      });
      mediaStreamRef.current = null;
    }
    
    // Disconnect and close the audio context
    if (processorNodeRef.current) {
      try {
        processorNodeRef.current.disconnect();
      } catch (e) {
        console.warn('Error disconnecting processor node:', e);
      }
      processorNodeRef.current = null;
    }
    
    if (analyserRef.current) {
      try {
        analyserRef.current.disconnect();
      } catch (e) {
        console.warn('Error disconnecting analyser node:', e);
      }
      analyserRef.current = null;
    }
    
    if (sourceNodeRef.current) {
      try {
        sourceNodeRef.current.disconnect();
      } catch (e) {
        console.warn('Error disconnecting source node:', e);
      }
      sourceNodeRef.current = null;
    }
    
    if (audioContextRef.current) {
      try {
        audioContextRef.current.close();
      } catch (e) {
        console.warn('Error closing audio context:', e);
      }
      audioContextRef.current = null;
    }
    
    // Clear WebSocket streaming flag
    webSocketStreamingRef.current = false;
  };

  // Get system audio (Electron only)
  const getSystemAudio = React.useCallback(async (): Promise<MediaStream | null> => {
    if (window.electron && window.electron.getSystemAudio) {
      try {
        // Call Electron's getSystemAudio function
        console.log('Getting system audio through Electron API');
        return await window.electron.getSystemAudio();
      } catch (error) {
        console.error('Error getting system audio:', error);
        return null;
      }
    } else {
      console.warn('System audio capture not available (Electron not detected)');
      return null;
    }
  }, []);
  
  const getAudioLevel = React.useCallback(() => {
    return recordingStatus?.isRecording ? audioLevelRef.current : 0;
  }, [recordingStatus?.isRecording]);

  const startRecording = async (title: string, description?: string): Promise<string | null> => {
    setIsLoading(true);
    setError(null);
    
    try {
      // Generate a new session ID
      const sessionId = `session-${Date.now()}`;
      
      console.log(`Starting recording for session ${sessionId}`);
      
      if (!selectedDevice) {
        setError('No audio device selected');
        return null;
      }
      
      // Create audio context and analyzer
      const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
      const audioCtx = new AudioContextClass({ sampleRate: 44100 });
      audioContextRef.current = audioCtx;
      
      // Create analyzer for audio levels
      const analyser = audioCtx.createAnalyser();
      analyser.fftSize = 256;
      analyserRef.current = analyser;
      
      // Initialize audio buffer for analyzer
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Float32Array(bufferLength);
      audioBufferRef.current = dataArray;
      
      // Set up microphone/audio input
      let stream;
      
      try {
        // Request audio input device
        const constraints = {
          audio: {
            deviceId: selectedDevice.deviceId ? { exact: selectedDevice.deviceId } : undefined,
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false
          }
        };
        
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        mediaStreamRef.current = stream;
        
        // Connect audio graph
        const source = audioCtx.createMediaStreamSource(stream);
        sourceNodeRef.current = source;
        
        // Create script processor for raw audio data
        const processor = audioCtx.createScriptProcessor(4096, 1, 1);
        processorNodeRef.current = processor;
        
        // Connect the audio graph
        source.connect(analyser);
        analyser.connect(processor);
        processor.connect(audioCtx.destination);
        
        // Reset audio chunks array
        audioChunksRef.current = [];
        
        // Make sure WebSocket is connected
        if (webSocketContext.connectionStatus !== 'open') {
          console.log('WebSocket not connected, attempting to reconnect...');
          webSocketContext.reconnect();
          
          // Wait for WebSocket to connect with timeout
          console.log('Waiting for WebSocket connection to establish...');
          let connectionAttempts = 0;
          const maxAttempts = 5;
          
          while (webSocketContext.connectionStatus !== 'open' && connectionAttempts < maxAttempts) {
            connectionAttempts++;
            console.log(`WebSocket connection attempt ${connectionAttempts}/${maxAttempts}...`);
            
            // Wait a moment for connection
            await new Promise(resolve => setTimeout(resolve, 1000));
            
            if (webSocketContext.connectionStatus === 'open') {
              console.log('WebSocket connection established successfully!');
              break;
            }
          }
        }
        
        // Final check - ensure connection is ready
        if (webSocketContext.connectionStatus !== 'open') {
          console.warn('WebSocket connection could not be established. Audio streaming may not work.');
          console.log('Will proceed with recording anyway. Please check network connectivity.');
        } else {
          console.log('WebSocket connected and ready to send audio data');
        }
        
        // Initialize WebSocket streaming
        webSocketStreamingRef.current = true;
        
        // Set up processing callback
        processor.onaudioprocess = onaudioprocess;
        
        console.log('Audio processing initialized with sample rate:', audioCtx.sampleRate);
        
        // Connect to session via WebSocket
        console.log('Connecting to session via WebSocket...');
        webSocketContext.connectToSession(sessionId);
        
        // Send initial recording start message
        const messageSent = webSocketContext.sendMessage({
          type: 'recording_start' as WebSocketMessageType,
          data: {
            sessionId,
            title,
            description,
            sampleRate: audioCtx.sampleRate,
            deviceInfo: {
              name: selectedDevice.name,
              isLoopback: selectedDevice.isLoopback
            }
          }
        });
        
        if (!messageSent) {
          console.warn('Failed to send recording_start message. Audio streaming may not work properly.');
        } else {
          console.log('Successfully sent recording_start message');
        }
      } catch (err) {
        console.error('Error accessing microphone:', err);
        setError('Failed to access microphone. Please check your permissions.');
        cleanupAudioProcessing();
        return null;
      }
      
      // Start recording status updates
      if (statusPollingRef.current) {
        clearInterval(statusPollingRef.current);
      }
      
      setRecordingStatus({
        isRecording: true,
        sessionId,
        startTime: new Date().toISOString(),
        duration: 0,
        audioLevel: 0.05,
        errorMessage: null,
      });
      
      // Update duration
      statusPollingRef.current = setInterval(() => {
        setRecordingStatus(prev => prev ? {
          ...prev,
          duration: prev.duration + 1,
          audioLevel: audioLevelRef.current
        } : null);
      }, 1000);
      
      return sessionId;
    } catch (err) {
      console.error('Error starting recording:', err);
      setError('Failed to start recording. Please try again.');
      cleanupAudioProcessing();
      return null;
    } finally {
      setIsLoading(false);
    }
  };

  const stopRecording = async (sessionId: string): Promise<boolean> => {
    // Handle special reset case
    if (sessionId === 'reset-phantom-recording') {
      console.log('Resetting phantom recording state');
      
      if (statusPollingRef.current) {
        clearInterval(statusPollingRef.current);
        statusPollingRef.current = null;
      }
      
      cleanupAudioProcessing();
      
      setRecordingStatus({
        isRecording: false,
        sessionId: null,
        startTime: null,
        duration: 0,
        audioLevel: 0,
        errorMessage: null,
      });
      
      return true;
    }
    
    // Normal case - stopping an actual recording
    if (!sessionId) {
      console.warn('No active recording session ID provided');
      setError('No active recording session');
      return false;
    }

    // Check if recording is active
    if (!recordingStatus?.isRecording) {
      console.warn('No active recording to stop');
      return false;
    }

    console.log(`Stopping recording for session ${sessionId}`);
    setIsLoading(true);
    setError(null);

    try {
      // Stop WebSocket streaming
      webSocketStreamingRef.current = false;
      
      // Clean up audio processing
      cleanupAudioProcessing();
      
      // Clean up any status polling
      if (statusPollingRef.current) {
        clearInterval(statusPollingRef.current);
        statusPollingRef.current = null;
      }
      
      // Tell backend to stop transcription
      try {
        await axios.post(`/api/sessions/${sessionId}/transcription/stop`);
      } catch (apiError) {
        console.error('Error stopping transcription on backend:', apiError);
        // Continue with cleanup even if API call fails
      }
      
      // Reset recording status
      setRecordingStatus({
        isRecording: false,
        sessionId: null,
        startTime: null,
        duration: 0,
        audioLevel: 0,
        errorMessage: null,
      });

      return true;
    } catch (err) {
      console.error('Error stopping recording:', err);
      setError('Failed to stop recording');
      return false;
    } finally {
      setIsLoading(false);
    }
  };

  const uploadAudio = async (file: File, title: string, description?: string): Promise<string | null> => {
    setIsLoading(true);
    setError(null);

    try {
      // Simulate file upload with delay
      console.log('Simulating audio upload...');
      const mockSessionId = `upload-${Date.now()}`;
      await new Promise(resolve => setTimeout(resolve, 1500));
      return mockSessionId;
    } catch (err) {
      console.error('Error uploading audio:', err);
      setError('Failed to upload audio file. Please check the file format and try again.');
      return null;
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <AudioContext.Provider
      value={{
        devices,
        selectedDevice,
        isLoading,
        error,
        recordingStatus,
        refreshDevices,
        selectDevice,
        startRecording,
        stopRecording,
        getAudioLevel,
        uploadAudio,
      }}
    >
      {children}
    </AudioContext.Provider>
  );
};
